---
title: "Linear Independence and Dependence, Inner Products, Orthogonal Sets, and Gram-Schmidt Orthogonalization."
author: "Brody Erlandson"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    toc_depth: 2
---

\include{mathCommands}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Linear Independence and Dependence

## Definitions and Theorems {.tabset}

### Definition 1.1.
**Relation of Linear Dependence**

Given a set of vectors 
$S = \{\mathbf{v}_1, \mathbf{v}_2, \dots , \mathbf{v}_n\}$.
A true equation of the form 
$c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \dots + c_n\mathbf{v}_n = \mathbf{0}$
is a *relation of linear dependence* on $S$.

### Definition 1.2. 
**Trivial Relation of Linear Dependence**

Given a set of vectors $S = \{\mathbf{v}_1, \mathbf{v}_2, \dots , \mathbf{v}_n\}$, the *trivial relation of linear dependence* on $S$ is $0\mathbf{v}_1 + 0\mathbf{v}_2 + \dots + 0\mathbf{v}_n = \mathbf{0}$.

### Definition 1.3. 
**Linearly Independent**

A set of vectors $S = \{\mathbf{v}_1, \mathbf{v}_2, \dots , \mathbf{v}_n\}$ is *linearly independent* if the only relation of linear dependence on $S$ is the trivial one.

### Definition 1.4. 
**Linearly Dependent**

If a set of vectors is not linearly independent, it is *linearly dependent*. In other words, a set $S = \{\mathbf{v}_1, \mathbf{v}_2, \dots , \mathbf{v}_n\}$ is linearly dependent if and only if the equation $c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \dots + c_n\mathbf{v}_n = \mathbf{0}$ has a non-trivial solution.

### Theorem 1.1. 
**Linearly Dependent Sets**

An indexed set $S = \{\mathbf{v}_1, \mathbf{v}_2, \dots , \mathbf{v}_n\}$ of two or more vectors is linearly dependent if and only if at least one of the vectors in $S$ is a linear combination of the others.

*Proof* Assume $S$ is linearly dependent. Then there exist scalars $c_{1}, c_{2}, \ldots, c_{p}$, such that

$$c_{1} \mathbf{v}_{1}+c_{2} \mathbf{v}_{2}+\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$$

and at least one of the scalars is not zero. Call it $c_{j}$. So we have

$$\begin{aligned}
c_{1} \mathbf{v}_{1}+c_{2} \mathbf{v}_{2}+\cdots+c_{j} \mathbf{v}_{j}+\cdots+c_{p} \mathbf{v}_{p} &=\mathbf{0} \\
c_{j} \mathbf{v}_{j} &=-c_{1} \mathbf{v}_{1}-c_{2} \mathbf{v}_{2}-\cdots-c_{p} \mathbf{v}_{p} \\
\mathbf{v}_{j} &=\left(-\frac{c_{1}}{c_{j}}\right) \mathbf{v}_{1}+\left(-\frac{c_{2}}{c_{j}}\right) \mathbf{v}_{2}+\cdots+\left(-\frac{c_{p}}{c_{j}}\right) \mathbf{v}_{p}
\end{aligned}$$

and $\mathbf{v}_{,}$is a linear combination of the other vectors in $S$.
Conversely, assume that $\mathbf{v}_{j}$ is a linear combination of the other vectors in $S$. Then we can write

$$\begin{aligned}
\mathbf{v}_{j} &=c_{1} \mathbf{v}_{1}+c_{2} \mathbf{v}_{2}+\cdots+c_{j-1} \mathbf{v}_{j-1}+c_{j+1} \mathbf{v}_{j+1}+\cdots+c_{p} \mathbf{v}_{p} \\
1\mathbf{v}_{j} &=c_{1} \mathbf{v}_{1}+c_{2} \mathbf{v}_{2}+\cdots+c_{j-1} \mathbf{v}_{j-1}+c_{j+1} \mathbf{v}_{j+1}+\cdots+c_{p} \mathbf{v}_{p} \\
\mathbf{0} &=c_{1} \mathbf{v}_{1}+c_{2} \mathbf{v}_{2}+\cdots+c_{j-1} \mathbf{v}_{j-1}-1 \mathbf{v}_{j}+c_{j+1} \mathbf{v}_{j+1}+\cdots+c_{p} \mathbf{v}_{p}
\end{aligned}$$

So

$$\sum_{i=1}^{p} c_{i} v_{i}=0$$

where $c_{j}=-1$, and $S$ is linearly dependent.


### Corollary 1.1.1. 

If an indexed set $S = \{\mathbf{v}_1, \mathbf{v}_2, \dots , \mathbf{v}_n\}$ of two or more vectors is linearly dependent, and $\mathbf{v}_1 \not= 0$, then one of the vectors, $\mathbf{v}_j$, where $j > 1$, is a linear combination of the preceding vectors, $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_{j-1}$.

### Theorem 1.2. 
**More Vectors Than Entries**

Any set of $p$ vectors in $\mathbb{C}^n$ is linearly dependent if $p > n$.

*Proof* Let $S = \{\mathbf{v}_1, \mathbf{v}_2, . . . , \mathbf{v}_p\}$ where $\mathbf{v}_i \in \mathbb{C}^n$, where $p > n$. Let

$$\mathbf{A} = \left[\mathbf{v}_1|\mathbf{v}_2|\dots|\mathbf{v}_p\right]$$

$\mathbf{A}$ is an $n \times p$ matrix which has more columns than rows, with at most $n$ pivots. Therefore the system corresponding to the matrix equation $\mathbf{Ax = 0}$ has at least one free variable, and infinitely many solutions. In particular, there is a nontrivial solution. Therefore the columns of $\mathbf{A}$ form a linearly dependent set.

### Theorem 1.3. 
**Zero Vector Makes Sets Dependent**

Any set of vectors in $\mathbb{C}^n$ that contains the zero vector is linearly dependent.

*Proof* Assume $S = \{\mathbf{v}_1, \mathbf{v}_2, . . . , \mathbf{v}_p\}$ where $\mathbf{v}_i \in \mathbb{C}^n$, and let $\mathbf{v}_j = \mathbf{0}$. Then

$$0\mathbf{v}_{1}+0\mathbf{v}_{2}+\dots+c_j\mathbf{v}_{j}+\dots+0\mathbf{v}_{p} = \mathbf{0}$$

where $c_j \not= 0$ and $c_i = 0$ given $i \not= j$.

### Theorem 1.4. 
**Linearly Independent Sets of Column Vectors**

Let $\mathbf{A}$ be an $m \times n$ matrix and let $\mathbf{B}$ be the reduced row-echelon form of $\mathbf{A}$. The following statements are either all true or all false:

1. The columns of a matrix $\mathbf{A}$ form a linearly independent set.
2. The homogeneous system $\mathcal{LS}(\mathbf{A}, \mathbf{0})$ has only the trivial solution.
3. Every column of $\mathbf{B}$ is a pivot column.
4. The homogeneous system $\mathcal{LS}(\mathbf{A}, \mathbf{0})$ has no free variables.

## Examples and Visualizations

for Visualizations show linear Independent and dependent through the span of individual or pairs of vectors.

Lets first start out with an easy one. Is the following linear independent?

$$S = \bigg\{ \begin{bmatrix}
                 1\\
                 0\\ 
           \end{bmatrix}, 
           \begin{bmatrix}
                 2\\
                 0\\
           \end{bmatrix}\bigg\} = \{\mathbf{v}_1,\mathbf{v}_2\}$$

<details><summary>Click here Solution</summary>  
           
It should be very easy to see that $\mathbf{v}_2 = 2 \mathbf{v}_1$, meaning $\mathbf{v}_2$ is a linear combination of $\mathbf{v}_1$. Thus $S$ is linear dependent, because there is the null space contains more than $\mathbf{0}$. We can also note that $\mathbf{v}_2 \in \langle \{ \mathbf{v}_1\}\rangle$

</details> \newline

Now lets look at one not so easy to see. Is the following linear independent?

$$S = \Bigg\{ \begin{bmatrix}
                 5\\
                 0\\ 
                 6
           \end{bmatrix}, 
           \begin{bmatrix}
                 1\\
                 1\\ 
                 5
           \end{bmatrix},
           \begin{bmatrix}
                 -3\\
                 2\\ 
                 7
           \end{bmatrix}\Bigg\} = \{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}$$ 
           
<details><summary>Click here Solution</summary> 

If we allow $\mathbf{A} = [\mathbf{v}_1|\mathbf{v}_2|\mathbf{v}_3]$, then 

$$\mathbf{A} \sim \begin{bmatrix}
                   1 & 0 & 0\\
                   0 & 1 & 0\\ 
                   0 & 0 & 1
                  \end{bmatrix}$$
                  
So, given Theorem 1.4., columns of $\mathbf{A}$ form a linearly independent set. This also means that $\langle S\rangle= \mathbb{C}^3$. Furthermore, meaning $\mathbf{v}_j \not= \sum_{i=1}^3 c_i\mathbf{v}_i, \text{ given } c_j = 0$.

</details> \newline

Lastly, is the following linear independent?

$$S = \Bigg\{ \begin{bmatrix}
                 5\\
                 0\\ 
                 6
           \end{bmatrix}, 
           \begin{bmatrix}
                 1\\
                 1\\ 
                 5
           \end{bmatrix},
           \begin{bmatrix}
                 6\\
                 1\\ 
                 11
           \end{bmatrix}\Bigg\} = \{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}$$ 
           
<details><summary>Click here Solution</summary> 

If we allow $\mathbf{A} = [\mathbf{v}_1|\mathbf{v}_2|\mathbf{v}_3]$, then 

$$\mathbf{A} \sim \begin{bmatrix}
                   1 & 0 & 1\\
                   0 & 1 & 1\\ 
                   0 & 0 & 0
                  \end{bmatrix}$$
                  
So, given Theorem 1.4., columns of $\mathbf{A}$ form a linearly dependent set. This also means that $\mathbf{v}_3 \in \langle \{\mathbf{v}_1, \mathbf{v}_2\}\rangle$. Furthermore, meaning $\mathbf{v}_3 = c_1\mathbf{v}_1 + c_2\mathbf{v}_2, \text{ for some } c_1,c_2 \not= 0$. We can visualize this span to get a better idea:

```{r, echo=FALSE, warning=FALSE}
#library(rgl)
#
#plot3d(c(0, 0, 0), col="yellow", xlim=c(-5,10), ylim=c(-5,10), zlim=c(-5,10))
#
#arrow3d(p0 = c(0, 0, 0), p1 = c(5, 0, 6), type="rotation", col="red")
#arrow3d(p0 = c(0, 0, 0), p1 = c(1, 1, 5), type="rotation", col="blue")
#arrow3d(p0 = c(0, 0, 0), p1 = c(6, 1, 11), type="rotation", col="black")
#planes3d(6, 19, -5, 0, col="purple", alpha=.3)
```

![](3dDepenSpan.png)

</details> \newline

# Inner Products 

## Definitions and Theorems {.tabset}

### Definition 2.1.
**Inner Product**

If $\mathbf{u},\mathbf{v} \in \mathbb{C}^n$, then the inner product of $\mathbf{u}$ and $\mathbf{v}$ is the complex number given by the following formula:

$\langle \mathbf{u}, \mathbf{v}\rangle = \sum^n_{i=1} \bar{u}_iv_i$

An inner product is also called a *scalar product*, emphasizing the fact that
the inner product of two vectors is a scalar. If restricting to $\mathbb{R}^n$, this is equivalent to the dot product.

### Theorem 2.1. 
**Properties of Inner Products**

If $\mathbf{u},\mathbf{v} \in \mathbb{C}^n$ and $c \in \mathbb{C}$, then the inner product satisfies the following properties:

1. $\langle \mathbf{u}, \mathbf{v}\rangle = \overline{\langle \mathbf{v}, \mathbf{u}\rangle}$
2. $\langle \mathbf{u} + \mathbf{v}, \mathbf{w}\rangle = \langle \mathbf{u}, \mathbf{w}\rangle + \langle \mathbf{v}, \mathbf{w}\rangle$
3. $\langle \mathbf{u}, \mathbf{v} + \mathbf{w}\rangle = \langle \mathbf{u}, \mathbf{v}\rangle + \langle \mathbf{u}, \mathbf{w}\rangle$
4. $\langle c\mathbf{u}, \mathbf{v}\rangle = \bar{c}\langle \mathbf{u}, \mathbf{v}\rangle$
5. $\langle \mathbf{u}, c\mathbf{v}\rangle = c\langle \mathbf{u}, \mathbf{v}\rangle$
6. $\langle \mathbf{u}, \mathbf{u}\rangle â‰¥ 0,\ \text{and}\ \langle \mathbf{u}, \mathbf{u}\rangle = 0\ \text{iff}\ \mathbf{u} = \mathbf{0}$

### Definition 2.2.
**Norm**

If $\mathbf{u} \in \mathbb{C}^n$, the norm of $\mathbf{u}$ is the complex number:

$$\lVert \mathbf{u} \rVert = \sqrt{\langle \mathbf{u}, \mathbf{u}\rangle} $$

### Theorem 2.2.
**Norm of a Scalar Multiple**

If $\mathbf{u} \in \mathbb{C}^n$ and $c \in \mathbb{C}$, then

$$\lVert c\mathbf{u} \rVert = |c|\lVert\mathbf{u}\rVert$$

*Proof* 

$$\norm{c\u} = \sqrt{\innerProd{c\u}{c\u}} = \sqrt{\bar{c}c\innerProd{\u}{\u}} \\ = \sqrt{\bar{c}c}\sqrt{\innerProd{\u}{\u}} = |c|\norm{\u}$$

### Theorem 2.3.
**Norm is Real**

If $\mathbf{u} \in \mathbb{C}^n$, then $\lVert\mathbf{u}\rVert \in \mathbb{R}$

*Proof*

$$\norm{\u} = \sqrt{\sum^n_{i=1} \bar{u}_iu_i} = \sqrt{\sum^n_{i=1} |u_i|^2} \in \R$$

### Theorem 2.4. 
**Modulus of Norm is Norm**

If $\mathbf{u} \in \mathbb{C}^n$, then $|\lVert\mathbf{u}\rVert| = \lVert\mathbf{u}\rVert$

*proof*

$$\norm{\u} \in \R \text{ and } \norm{\u} \ge 0, \text{ so the modulus is the absolute value of a positive real number, thus } |\norm{\u}| = \norm{\u}$$

### Definition 1.3. 
**Distance**

The distance between two vectors $\mathbf{u}, \mathbf{v} \in \mathbb{C}^n \  \text{is}\  \lVert\mathbf{u} - \mathbf{v}\rVert$

### Definition 2.4. 
**Orthogonal**

Vectors $\mathbf{u}, \mathbf{v} \in \mathbb{C}^n$ are orthogonal if $\langle \mathbf{u}, \mathbf{v}\rangle = 0$. If $\mathbf{u}$ and $\mathbf{v}$ are orthogonal, we write $\mathbf{u} \perp \mathbf{v}$

## Examples and Visualizations

Given,

$$\v = \vddd{7+1i}{2+0i}{1-5i}, \u = \vddd{0+2i}{6-1i}{2-1i}$$

find $\innerProd{\v}{\u}$.

<details><summary>Click here Solution</summary>

$$\innerProd{\v}{\u} = \overline{(7+1i)}(0+2i) + \overline{(2+0i)}(6-1i) + \overline{(1-5i)}(2-1i) \\ = (7-1i)(0+2i) + (2-0i)(6-1i) + (1+5i)(2-1i) \\ = 21(1+i)$$

</details> \newline

Now let,

$$\v = \vdd{\frac{1}{2}}{\frac{\sqrt{3}}{2}}, \u = \vdd{0}{1}$$

find $\norm{\v}$ and $\norm{\u}$.

<details><summary>Click here Solution</summary>

$$\norm{\v} = \sqrt{(\frac{1}{2})^2 + (\frac{\sqrt{3}}{2})^2} = 1$$

and

$$\norm{\u} = \sqrt{(0)^2 + (1)^2} = 1$$

We can visualize this pretty easily, because both of these are on the unit circle.

</details> \newline

Lastly, using $\u$ and $\v$ from the previous question, find the distance between the two vectors.

<details><summary>Click here Solution</summary>

$$\norm{\u - \v} = \sqrt{(\frac{1}{2})^2 + (1 - \frac{\sqrt{3}}{2})^2} = \sqrt{2-\sqrt3}$$

</details> \newline


# Orthogonal Sets 

## Definitions and Theorems {.tabset}

### Definition 3.1.
**Orthogonal Set**

$\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_p\},\ \text{where}\ \mathbf{u}_i \in \mathbb{C}^n$, is orthogonal if the vectors in the set are pairwise orthogonal. That is, $\langle\mathbf{u}_i, \mathbf{u}_j\rangle = 0\ \text{for}\ i \not= j$.

### Theorem 3.1.

An orthogonal set of non-zero vectors in $\mathbb{C}^n$ is linearly independent.

*proof* Let $S = \{\u_1, \u_2, \dots, \u_p\}$ be an orthogonal set of non-zero vectors. Assume S is linearly dependent. Then at least one of the vectors in the set can be written as a linear combination of the others. Without loss of generality, let that vector be $\u_1$. Then

$$ \u_1 = \sum_{i=2}^p c_i\u_i \\ \innerProd{\u_1}{\u_1} = \innerProd{\u_1}{\sum_{i=2}^p c_i\u_i} \\ \innerProd{\u_1}{\u_1} = \sum_{i=2}^p c_i\innerProd{\u_1}{\u_i} \\ \innerProd{\u_1}{\u_1} = \sum_{i=2}^p c_i0 = 0$$

However, $\innerProd{\u_1}{\u_1} = 0$ iff $\u_1 = 0$, which is a contradiction.

### Definition 3.2. 
**Unit Vector**

If $\lVert\mathbf{u}\rVert = 1$, then $\mathbf{u}$ is a *unit vector*.

### Theorem 3.2.
**Normalizing**

If $\mathbf{v} \in \mathbb{C}^n$, then $\mathbf{u} = \frac{1}{\lVert\mathbf{v}\rVert} \mathbf{v}$ is a unit vector. Multiplying a vector by the reciprocal of its norm to create a unit vector is called *normalizing* the vector.

*proof* 

$$\norm{\frac{1}{\norm{\v}}\v} = \frac{1}{\norm{\v}}\norm{\v} = 1$$

### Definition 3.3.
**Orthonormal Set**

An orthogonal set of unit vectors is called an *orthonormal set*

## Visualizations and Examples

Lets start with a set of 3 vectors:

$$S = \Bigg\{\vddd{1}{2}{3}, \vddd{3}{0}{-1}, \vddd{1}{-2}{1} \Bigg\} = \{\v_1,\v_2,\v_3\}$$

Normalize these vectors and check if this is an orthogonal set.

<details><summary>Click here Solution</summary>

To normalize the vectors, we need to divide by its norm. $\norm{\v_1} = \sqrt{1^2+2^2+3^2} = \sqrt{14}$, $\norm{\v_2} = \sqrt{10}$, and $\norm{\v_3} = \sqrt{6}$. Giving us:

$$\Bigg\{\vddd{\frac{1}{\sqrt{14}}}{\frac{2}{\sqrt{14}}}{\frac{3}{\sqrt{14}}}, \vddd{\frac{3}{\sqrt{10}}}{0}{\frac{-1}{\sqrt{10}}}, \vddd{\frac{1}{\sqrt6}}{\frac{-2}{\sqrt6}}{\frac{1}{\sqrt6}} \Bigg\}$$

However to find if they are a orthogonal set, we have to check $\innerProd{\v_1}{\v_2}=\innerProd{\v_2}{\v_3}=\innerProd{\v_3}{\v_1}=0$. We will check one here and leave the others as an exercise. $\innerProd{\v_1}{\v_2} = 1*3+ 0*2-1*3 = 0$. We can also visualize what the normalization has done, and what orthogonal vectors look like:

```{r, echo=FALSE, warning=FALSE}
#library(rgl)
#
#plot3d(c(0, 0, 0), col="yellow", xlim=c(-1,1), ylim=c(-1,1), zlim=c(-1,1))
#
#arrow3d(p0 = c(0, 0, 0), p1 = c(1/sqrt(14), 2/sqrt(14), 3/sqrt(14)), type="rotation", #col="red")
#arrow3d(p0 = c(0, 0, 0), p1 = c(3/sqrt(10),0,-1/sqrt(10)), type="rotation", col="blue")
#arrow3d(p0 = c(0, 0, 0), p1 = c(1/sqrt(6), -2/sqrt(6), 1/sqrt(6)), type="rotation", #col="black")
#
#arrow3d(p0 = c(0, 0, 0), p1 = c(1, 2, 3), type="rotation", col="red", alpha=.3)
#arrow3d(p0 = c(0, 0, 0), p1 = c(3,0,-1), type="rotation", col="blue", alpha=.3)
#arrow3d(p0 = c(0, 0, 0), p1 = c(1, -2, 1), type="rotation", col="black", alpha=.3)
#
#spheres3d(0,0,0, col="grey", alpha=.3)
```

![](orthoNormalSet.png)

</details> \newline

Is it possible to have an orthogonal set that has more vectors than the number of dimensions?

<details><summary>Click here Solution</summary>

Say we have a set with $m$ vectors in $\C^n$, then $m \le n$ for it to be an orthogonal set.

</details> \newline

# Gram-Schmidt Orthogonalization

## Definitions and Theorems {.tabset}

### Theorem 4.1. 
**Gram-Schmidt Orthogonalization**

Given a linearly independent set of vectors in $\mathbb{C}^{n}, S=\left\{\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{p}\right\}$, define
$$\mathbf{u}_{j}=\mathbf{v}_{j}-\sum_{i=1}^{j-1} \frac{\left\langle\mathbf{u}_{i}, \mathbf{v}_{j}\right\rangle}{\left\langle\mathbf{u}_{i}, \mathbf{u}_{i}\right\rangle} \mathbf{u}_{i}, j=1, \ldots, p$$
Then the set $T=\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{p}\right\}$ is an orthogonal set of non-zero vectors, and $\langle T\rangle=\langle S\rangle .$
Lets expand the sum for clarity:
$$\mathbf{u}_{j}=\mathbf{v}_{j}-\frac{\left\langle\mathbf{u}_{1}, \mathbf{v}_{j}\right\rangle}{\left\langle\mathbf{u}_{1}, \mathbf{u}_{1}\right\rangle} \mathbf{u}_{1}-\frac{\left\langle\mathbf{u}_{2}, \mathbf{v}_{j}\right\rangle}{\left\langle\mathbf{u}_{2}, \mathbf{u}_{2}\right\rangle} \mathbf{u}_{2}-\frac{\left\langle\mathbf{u}_{3}, \mathbf{v}_{j}\right\rangle}{\left\langle\mathbf{u}_{3}, \mathbf{u}_{3}\right\rangle} \mathbf{u}_{3}-\cdots-\frac{\left\langle\mathbf{u}_{j-1}, \mathbf{v}_{j}\right\rangle}{\left\langle\mathbf{u}_{j-1}, \mathbf{u}_{j-1}\right\rangle} \mathbf{u}_{j-1}, \quad j=1, \ldots, p$$

## Visualizations and Examples

Using,

$$S = \Bigg\{\vddd{1}{1}{1}, \vddd{1}{1}{0}, \vddd{1}{0}{0}\Bigg\}$$

apply the Gram-Schmidt Orthogonalization to make an orthogonal set.

<details><summary>Click here Solution</summary>

First,

$$\u_1 = \vddd{1}{1}{1}$$

Next,

$$\u_2 = \vddd{1}{1}{0} - \frac{2}{3}\vddd{1}{1}{1} = \vddd{\frac{1}{3}}{\frac{1}{3}}{-\frac{2}{3}}$$

Lastly,

$$\u_2 = \vddd{1}{0}{0} - \frac{1}{3}\vddd{1}{1}{1} - \frac{1/3}{2/3}\vddd{\frac{1}{3}}{\frac{1}{3}}{-\frac{2}{3}} = \vddd{\frac{1}{2}}{-\frac{1}{2}}{0}$$

Now we can visualize what happened to the vectors:

```{r, echo=FALSE, warning=FALSE}
#library(rgl)
#
#plot3d(c(0, 0, 0), col="yellow", xlim=c(-1,1), ylim=c(-1,1), zlim=c(-1,1))
#
#arrow3d(p0 = c(0, 0, 0), p1 = c(1, 1, 1), type="rotation", col="red")
#arrow3d(p0 = c(0, 0, 0), p1 = c(1/3,1/3,-2/3), type="rotation", col="blue")
#arrow3d(p0 = c(0, 0, 0), p1 = c(1/2,-1/2,0), type="rotation", col="black")
#
#arrow3d(p0 = c(0, 0, 0), p1 = c(1, 1, 1), type="rotation", col="red", alpha=.3)
#arrow3d(p0 = c(0, 0, 0), p1 = c(1,1,0), type="rotation", col="blue", alpha=.3)
#arrow3d(p0 = c(0, 0, 0), p1 = c(1,0,0), type="rotation", col="black", alpha=.3)
#
#spheres3d(0,0,0, col="grey", alpha=.3)
```

![](3dGSP.png)

</details> \newline
